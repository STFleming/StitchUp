\subsection{Soft Errors}
A Soft Error or Single Event Upset is a random non-catastrophic error, where radiation causes
a perturbation to a circuit by temporarily altering a single signal or datum. 
Detecting and mitigating such errors is highly important in some fields, such as satellite design, 
where high design costs, harsh environments, and difficulties to repair once deployed make it necessary. 
However these concerns are no longer restricted to niche fields
since the shrinking of both device feature sizes and operating voltages make it increasingly likely 
that ground level radiation sources can cause soft errors.
This effect has been known about for some time \cite{normand1996single} and is related to 
the reduction in critical charge required to cause an upset, this in turn broadens the spectrum of
particles able to cause upsets such as muons with lower ionising power that are prevelant in terrestrial
environments.
Consequently, this concern is driving reliability as a first-class design constraint in the 
development of digital systems.

%This feels more like an our work section and less like background
In this work we shall focus on the effects of \textbf{single} errors, where we assume that only one SEU 
can occur per clock cycle.
We shall also limit ourselves to SRAM based FPGA designs, although the protection scheme 
presented in this paper can be equally applied to VLSI designs.

In an FPGA design circuit descriptions are mapped into a collection of Logic Blocks, decribed in programmable look-up-tables,
which are routed together via programmable switchboxes.
Configuration data for both the look-up-tables and switchboxes are stored in a configuration memory, which means soft errors
can either change the functionality of Logic Blocks or can change the routing between them.  
Block memory and flip-flops are also present in FPGA devices, where soft errors in these regions can alter the state of the circuit.
For this paper our \emph{fault model} assumes that a single soft error can occur at most every clock cycle, and that this error can effect block memory, configuration memory, and flip-flops. 
However our fault model does not include routing to and from our protected region to external I/O, such as DDR accesses.

\subsection{High level approaches to detecting soft errors}
%Traditional redundancy approaches
Dual Modular Redundancy (DMR) is the traditional approach for detecting faults,
where any difference between duplicated versions of components operating with the same state indicates 
an error.
Typically this is performed with two identical hardware units executing in lock step and
periodically comparing their outputs.

Redundancy is often not feasible in constrained systems such as satellites
since it is inherently costly consuming area, power, time, or all of the above.
For this reason researchers have been interested in finding methods to
increase reliability with less redundancy but without increasing engineering effort.
The majority of research into this problem has primarily focussed on low level approaches,
with only a few investigations into how this can be achieved through HLS tools.

%Intro to HLS approaches
Early work such as \cite{antola1998high} examined how hardware redundancy 
could be reduced when using HLS to generate DMR circuits.
The authors present a scheduling and alloction algorithm, where the circuit duplication is optimised
so that idle resources are reused instead of instantiating multiple hardware copies.
They manage to successfully reduce the area requirments, while avoiding error aliasing.

%Libary based approaches for Allocation stage of HLS tools
More recent work has instead of using redundancy examined how reliability can be optimised by 
selecting different implementations of components during allocation.
In \cite{tosun2005reliability}
a library of components annotated with area, power, and reliability metrics is used during component selection.    
Initially the set of most reliable components is selected, and then itterative optimisation passes attempt to
meet constraints on the other metrics in a fashion similar to power-aware HLS approaches. 

In \cite{glass2007interactive} this library approach is extended to include the replication of some components 
and an evolutionary algorithm is used to present the results to the user as a design space exploration.
As work in \cite{hara2013cost} extends it to encorporate reliability decisions into scheduling, arguing that
the longer a functional unit is active the more likely it will experience a fault. 

%Application level protection, what can we skip?
The methods discussed so far all propose changes to the underlying HLS algorithms,
none of them  investigate how the input source can be analysed beforehand to extract
critical and non-critical sections of code.
But do such critical and non-critical sections exist?
A large number of studies within the software domain have explored the resillience of various applications
to the injection of soft errors, and found that there are clear regions of code that
are more critical than others LIST OF CITATIONS HERE.

In \cite{wong2006soft} the authors examined probabilistic
inference applications which are designed to be robust against noisy incomplete input
data to see if this also translated to resilience to soft errors.
Instruction level fault injection techniques were used to discover that such algorithms
have a natural ability to mask multiple data errors, however control flow errors were more
severe and tended to result in premature program termination.

Within the field of approximate computing there has also been interest in separating code into
critical and non-critical sections.  
A notable example is EnerJ \cite{sampson2011enerj} where type
annotations indicate whether code regions should be precise or imprecise.
The type system then ensures the segregation of imprecise code from precise code allowing for the safe execution of imprecise code on approximate hardware.
Flikker \cite{liu2012flikker} has a similar approach where type-annotations are used to store data in
DRAM banks that are refreshed below the recomended manufacturers rate to save idle power consumption.  
For a selection of mobile application benchmarks they find that large portions of memory can be safely
stored in the low refresh DRAM with minimal impact on application results.

%HLS methods that exploit the static analysis of input code
Other recent work includes \cite{chen2014reliability} where the vulnerability of variables is analysed and the most vulnerable
variables for a HLS program are protected by assigning them to special registers.
The proposed variable vulnerability analysis takes into account their lifetimes, dependencies, and branch probabilities.
Through analysing the CDFG, error and branch probabilities are used to propagate error likelihood forward to estimate the total
vulnerability for each variable in the design.
A Integer Linear Program solver is then used to minimise the subset of protected registers to cover the maximum number of variables.
They build their results on top of LegUp, and use LLVM to perform the variable vulnerability analysis.
They achieve impressive results, where protecting 20\% of the registers results in more than 60\% soft error mitigation in the best case.
However full fault injection tests are not performed on the generated hardware, and only state-based probabilistic error testing
was performed.\\

%What we are aiming to do differently.
