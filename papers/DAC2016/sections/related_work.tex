%RELATED WORK SECTION
%Intro to HLS approaches
Early work such as \cite{antola1998high} examined how hardware redundancy 
could be reduced when using HLS to generate DMR circuits.
The authors present a scheduling and alloction algorithm, where the circuit duplication is optimised
so that idle resources are reused instead of instantiating multiple hardware copies.
They manage to successfully reduce the area requirments, while avoiding error aliasing.

%Libary based approaches for Allocation stage of HLS tools
More recent work has instead of using redundancy examined how reliability can be optimised by 
selecting different implementations of components during allocation.
In \cite{tosun2005reliability}
a library of components annotated with area, power, and reliability metrics is used during component selection.    
Initially the set of most reliable components is selected, and then itterative optimisation passes attempt to
meet constraints on the other metrics in a fashion similar to power-aware HLS approaches. 

In \cite{glass2007interactive} this library approach is extended to include the replication of some components 
and an evolutionary algorithm is used to present the results to the user as a design space exploration.
As work in \cite{hara2013cost} extends it to encorporate reliability decisions into scheduling, arguing that
the longer a functional unit is active the more likely it will experience a fault. 

%Application level protection, what can we skip?
The methods discussed so far all propose changes to the underlying HLS algorithms,
none of them  investigate how the input source can be analysed beforehand to extract
critical and non-critical sections of code.
But do such critical and non-critical sections exist?

%HLS methods that exploit the static analysis of input code
Other recent work includes \cite{chen2014reliability} where the vulnerability of variables is analysed and the most vulnerable
variables for a HLS program are protected by assigning them to special registers.
The proposed variable vulnerability analysis takes into account their lifetimes, dependencies, and branch probabilities.
Through analysing the CDFG, error and branch probabilities are used to propagate error likelihood forward to estimate the total
vulnerability for each variable in the design.
A Integer Linear Program solver is then used to minimise the subset of protected registers to cover the maximum number of variables.
They build their results on top of LegUp, and use LLVM to perform the variable vulnerability analysis.
They achieve impressive results, where protecting 20\% of the registers results in more than 60\% soft error mitigation in the best case.
However full fault injection tests are not performed on the generated hardware, and only state-based probabilistic error testing
was performed.\\

%What we are aiming to do differently.
