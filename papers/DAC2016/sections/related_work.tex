The majority of reliable HLS approaches to date have come from the development
of ASICs, and generally revolve around the idea of modifying the
allocation, scheduling, and binding stages to include a library of
components with different reliability criteria \cite{tosun2005reliability}\cite{glass2007interactive}\cite{hara2013cost}.
This library is then searched to determine the maximum possible reliability while
meeting other constraints. We believe that our work is complimentary to these approaches,
as we can protect the control-flow structure for a more reliable configuration of components.

More recent approaches have used static analysis techniques to analyse the
vulnerabilities of registers based on their execution lifetimes\cite{chen2014reliability}.
While this approach is effective, it mainly targets ASIC designs, where register
vulnerability is the largest concern. In FPGA devices this is not the case since the
SRAM configuration memory is the most vulnerable region with register soft errors
contributing very little.

Shastri et al. in \cite{shastri2015scheduling} examined how HLS for FPGA reliability could
be achieved by providing an allocation and scheduling algorithm which took into account the
reuse and redundancy of certain shared resources.
Again our approach is complimentary to this one, since it could be used to
improve the reliability of the data path while we protect the control-flow structure.

%RELATED WORK SECTION
%Intro to HLS approaches
%Early work such as \cite{antola1998high} examined how hardware redundancy 
%could be reduced when using HLS to generate DMR circuits.
%The authors present a scheduling and alloction algorithm, where the circuit duplication is optimised
%so that idle resources are reused instead of instantiating multiple hardware copies.
%They manage to successfully reduce the area requirments, while avoiding error aliasing.
%
%%Libary based approaches for Allocation stage of HLS tools
%More recent work has instead of using redundancy examined how reliability can be optimised by 
%selecting different implementations of components during allocation.
%In \cite{tosun2005reliability}
%a library of components annotated with area, power, and reliability metrics is used during component selection.    
%Initially the set of most reliable components is selected, and then itterative optimisation passes attempt to
%meet constraints on the other metrics in a fashion similar to power-aware HLS approaches. 
%
%In \cite{glass2007interactive} this library approach is extended to include the replication of some components 
%and an evolutionary algorithm is used to present the results to the user as a design space exploration.
%As work in \cite{hara2013cost} extends it to encorporate reliability decisions into scheduling, arguing that
%the longer a functional unit is active the more likely it will experience a fault. 
%
%%Application level protection, what can we skip?
%The methods discussed so far all propose changes to the underlying HLS algorithms,
%none of them  investigate how the input source can be analysed beforehand to extract
%critical and non-critical sections of code.
%But do such critical and non-critical sections exist?
%
%%HLS methods that exploit the static analysis of input code
%Other recent work includes \cite{chen2014reliability} where the vulnerability of variables is analysed and the most vulnerable
%variables for a HLS program are protected by assigning them to special registers.
%The proposed variable vulnerability analysis takes into account their lifetimes, dependencies, and branch probabilities.
%Through analysing the CDFG, error and branch probabilities are used to propagate error likelihood forward to estimate the total
%vulnerability for each variable in the design.
%A Integer Linear Program solver is then used to minimise the subset of protected registers to cover the maximum number of variables.
%They build their results on top of LegUp, and use LLVM to perform the variable vulnerability analysis.
%They achieve impressive results, where protecting 20\% of the registers results in more than 60\% soft error mitigation in the best case.
%However full fault injection tests are not performed on the generated hardware, and only state-based probabilistic error testing
%was performed.\\

%What we are aiming to do differently.
